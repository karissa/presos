how we make data work for individuals and communities

Unfortunate side-effect of open data: silos.

look at sue.

sue is a data analyst. she finds data from the internet and makes sense out of it.

ok, sue wants to do a study on water quality around the world.

first, she needs to get data about the quality of water. catalogue it. and analyze it.

does searches.
ckan portals -> “water quality” -> copy link -> download
socrata portals -> craft SOPA query -> wait -> download
custom portals -> “click click click” -> copy link -> download

etc..

once the data is all downloaded, we can make awesome charts, graphs, analysis, policy papers, news articles, everything!

… but what happens if the data changes? sue will have to repeat the process in order to find out
…. what if the data becomes unavailable? no one else can replicate it.

Analyses based on faulty data could introduce faulty decisions into political-economic and community processes

To fix this, we will build a search engine that will crawl data sources.
users will be able to search for data across many portals and other data sources with a single query
an accompanying desktop application will keep track of previous downloads.
once the source data changes, publicbits.org will notify the user to download the newest copy

we do this by crafting a new design for open data.
Our approach is instead inspired by BitTorrent, but adapted to work for open data.
after users download, they can offer unused bandwidth to upload their previously downloaded datasets as a public services.
if the original source goes offline, the data will still be available.

We’d love to see adoption in major newsrooms, political advocacy organizations, and research labs. Users would initially be able to manage their datasets locally, but ideally this tool could be used to package metadata and send it to colleagues, so that others could use the same data environment in different places.


